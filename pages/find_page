#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import random
from datetime import datetime, timedelta, timezone

import streamlit as st
import pandas as pd
import altair as alt

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

st.set_page_config(
    page_title="YouTubeê²€ìƒ‰ê¸°",
    page_icon="ğŸ”",
    layout="wide",
)

st.markdown(
    """
<a id="page_top"></a>
<style>
.block-container { padding-top: 3rem !important; }
[data-testid="stDataFrame"] button[kind="icon"] {
    display: none !important;
}
</style>
""",
    unsafe_allow_html=True,
)

CONFIG_PATH = "config.json"

def load_config():
    if not os.path.exists(CONFIG_PATH):
        return {"quota_usage": {}, "keyword_log": []}
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        data = {}
    if "quota_usage" not in data:
        data["quota_usage"] = {}
    if "keyword_log" not in data:
        data["keyword_log"] = []
    return data

def save_config(data: dict):
    try:
        with open(CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.warning(f"config.json ì €ì¥ ì˜¤ë¥˜: {e}")

CONFIG_DATA = load_config()

KST = timezone(timedelta(hours=9))

COUNTRY_LANG_MAP = {
    "ë¯¸êµ­": ("US", "en"),
    "ì˜êµ­": ("GB", "en"),
    "í•œêµ­": ("KR", "ko"),
    "ì¼ë³¸": ("JP", "ja"),
    "ì¸ë„": ("IN", "en"),
    "ë¸Œë¼ì§ˆ": ("BR", "pt"),
    "ìºë‚˜ë‹¤": ("CA", "en"),
    "ë…ì¼": ("DE", "de"),
    "í”„ë‘ìŠ¤": ("FR", "fr"),
    "ë©•ì‹œì½”": ("MX", "es"),
    "í˜¸ì£¼": ("AU", "en"),
    "ìŠ¤í˜ì¸": ("ES", "es"),
    "ì´íƒˆë¦¬ì•„": ("IT", "it"),
    "ë„¤ëœë€ë“œ": ("NL", "nl"),
    "í„°í‚¤": ("TR", "tr"),
    "ì¸ë„ë„¤ì‹œì•„": ("ID", "id"),
    "íƒœêµ­": ("TH", "th"),
    "ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„": ("SA", "ar"),
    "ì•„ëì—ë¯¸ë¦¬íŠ¸": ("AE", "ar"),
}
COUNTRY_LIST = list(COUNTRY_LANG_MAP.keys())

TREND_CATEGORY_MAP = {
    "ì „ì²´": None,
    "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜": "1",
    "ìë™ì°¨/êµí†µ": "2",
    "ìŒì•…": "10",
    "ìŠ¤í¬ì¸ ": "17",
    "ê²Œì„": "20",
    "ì¸ë¬¼/ë¸”ë¡œê·¸": "22",
    "ì½”ë¯¸ë””": "23",
    "ì—”í„°í…Œì¸ë¨¼íŠ¸": "24",
    "ë‰´ìŠ¤/ì •ì¹˜": "25",
    "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼": "26",
    "êµìœ¡": "27",
    "ê³¼í•™/ê¸°ìˆ ": "28",
}

def get_current_api_key() -> str:
    keys = st.secrets.get("YOUTUBE_API_KEYS")
    if isinstance(keys, list) and keys:
        return str(keys[0]).strip()
    if isinstance(keys, str) and keys.strip():
        first = keys.strip().splitlines()[0]
        return first.strip()
    single = st.secrets.get("YOUTUBE_API_KEY")
    if isinstance(single, str) and single.strip():
        return single.strip()
    return ""

def get_youtube_client():
    key = get_current_api_key()
    if not key:
        raise RuntimeError(
            "YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "â–¶ .streamlit/secrets.toml ì— YOUTUBE_API_KEYS ë˜ëŠ” YOUTUBE_API_KEY ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )
    try:
        return build("youtube", "v3", developerKey=key, cache_discovery=False)
    except TypeError:
        return build("youtube", "v3", developerKey=key)

def quota_today_key():
    return datetime.now(KST).strftime("%Y-%m-%d")

def _load_quota_map():
    return CONFIG_DATA.get("quota_usage", {})

def _save_quota_map(data: dict):
    CONFIG_DATA["quota_usage"] = data
    save_config(CONFIG_DATA)

def get_today_quota_total() -> int:
    data = _load_quota_map()
    return int(data.get(quota_today_key(), 0))

def add_quota_usage(units: int):
    if units <= 0:
        return
    data = _load_quota_map()
    key = quota_today_key()
    data[key] = int(data.get(key, 0)) + int(units)
    _save_quota_map(data)

def _load_keyword_log():
    return CONFIG_DATA.get("keyword_log", [])

def _save_keyword_log(entries: list):
    CONFIG_DATA["keyword_log"] = entries
    save_config(CONFIG_DATA)

def append_keyword_log(query: str):
    q = (query or "").strip()
    if not q:
        return
    entries = _load_keyword_log()
    now = datetime.now(KST).isoformat(timespec="seconds")
    entries.append({"ts": now, "q": q})
    _save_keyword_log(entries)

def get_recent_keywords(limit: int = 30):
    entries = _load_keyword_log()
    out = []
    for item in entries:
        ts = item.get("ts")
        q = item.get("q")
        if not ts or not q:
            continue
        try:
            dt = datetime.fromisoformat(ts)
        except Exception:
            continue
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=KST)
        out.append((dt, q))
    out.sort(key=lambda x: x[0], reverse=True)
    return out[:limit]

def parse_published_at_to_kst(published_iso: str) -> datetime:
    dt_utc = datetime.fromisoformat(published_iso.replace("Z", "+00:00"))
    return dt_utc.astimezone(KST)

def human_elapsed_days_hours(later: datetime, earlier: datetime) -> tuple[int, int]:
    delta = later - earlier
    if delta.total_seconds() < 0:
        return 0, 0
    days = delta.days
    hours = delta.seconds // 3600
    return days, hours

def published_after_from_label(label: str):
    label = label.strip()
    now_utc = datetime.now(timezone.utc)
    if label == "ì œí•œì—†ìŒ":
        return None
    if label.endswith("ì¼"):
        days = int(label[:-1])
        dt = now_utc - timedelta(days=days)
    else:
        return None
    return dt.isoformat(timespec="seconds").replace("+00:00", "Z")

def parse_duration_iso8601(iso_dur: str) -> int:
    h = m = s = 0
    if not iso_dur or not iso_dur.startswith("PT"):
        return 0
    num = ""
    for ch in iso_dur[2:]:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num)
                num = ""
            elif ch == "M" and num:
                m = int(num)
                num = ""
            elif ch == "S" and num:
                s = int(num)
                num = ""
    return h * 3600 + m * 60 + s

def format_duration_hms(seconds: int) -> str:
    if seconds <= 0:
        return "0:00"
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"

def duration_filter_ok(seconds: int, label: str) -> bool:
    if label == "ì „ì²´":
        return True
    if label == "ì‡¼ì¸ ":
        return seconds < 60
    if label == "ë¡±í¼":
        return seconds >= 60
    if label == "1~20ë¶„":
        return 60 <= seconds < 20 * 60
    if label == "20~40ë¶„":
        return 20 * 60 <= seconds < 40 * 60
    if label == "40~60ë¶„":
        return 40 * 60 <= seconds < 60 * 60
    if label == "60ë¶„ì´ìƒ":
        return seconds >= 60 * 60
    return True

def parse_min_views(text: str) -> int:
    digits = text.replace(",", "").replace(" ", "").replace("ë§Œ", "0000")
    try:
        return int(digits)
    except Exception:
        return 0

def calc_grade(clicks_per_hour: int) -> str:
    v = clicks_per_hour
    if v >= 5000:
        return "A"
    if v >= 2000:
        return "B"
    if v >= 1000:
        return "C"
    if v >= 500:
        return "D"
    if v >= 300:
        return "E"
    if v >= 100:
        return "F"
    if v >= 50:
        return "G"
    return "H"

def search_videos(
    query: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)

    cost_used = 0
    max_fetch = max(1, min(int(max_fetch or 100), 5000))

    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            q=query,
            part="id",
            type="video",
            maxResults=take,
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            search_response = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"Search API ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in search_response.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items = video_response.get("items", [])
        for item in items:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append(
                {
                    "title": title,
                    "views": view_count,
                    "published_at_iso": published_at_iso,
                    "url": url,
                    "duration_sec": duration_sec,
                    "channel_title": snip.get("channelTitle", ""),
                    "thumbnail_url": thumb_url,
                }
            )

        fetched += len(page_ids)
        next_token = search_response.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used

def search_channels_by_keyword(
    keyword: str,
    max_results: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(max_results, 50))
    kwargs = dict(
        q=keyword,
        part="id",
        type="channel",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if lang_code:
        kwargs["relevanceLanguage"] = lang_code

    try:
        search_response = youtube.search().list(**kwargs).execute()
        cost_used = 100
    except HttpError as e:
        raise RuntimeError(f"Channel search API ì˜¤ë¥˜: {e}")

    ch_ids = [
        it["id"]["channelId"]
        for it in search_response.get("items", [])
        if "id" in it and "channelId" in it["id"]
    ]
    if not ch_ids:
        return [], cost_used

    try:
        ch_resp = youtube.channels().list(
            part="snippet,statistics",
            id=",".join(ch_ids),
        ).execute()
        cost_used += 1
    except HttpError as e:
        raise RuntimeError(f"Channels API ì˜¤ë¥˜: {e}")

    results = []
    for c in ch_resp.get("items", []):
        cid = c.get("id", "")
        sn = c.get("snippet", {}) or {}
        stt = c.get("statistics", {}) or {}
        subs = (
            int(stt.get("subscriberCount", 0))
            if stt.get("subscriberCount") is not None
            else None
        )
        total_views = int(stt.get("viewCount", 0))
        videos = int(stt.get("videoCount", 0))
        url = f"https://www.youtube.com/channel/{cid}" if cid else ""

        thumbs = sn.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append(
            {
                "channel_title": sn.get("title", ""),
                "subs": subs,
                "total_views": total_views,
                "videos": videos,
                "url": url,
                "thumbnail_url": thumb_url,
            }
        )

    results.sort(key=lambda r: (r["subs"] or 0), reverse=True)
    return results, cost_used

def search_videos_in_channel_by_name(
    channel_name: str,
    min_views: int,
    api_period_label: str,
    duration_label: str,
    max_fetch: int,
    region_code: str | None,
    lang_code: str | None,
):
    youtube = get_youtube_client()
    published_after = published_after_from_label(api_period_label)
    cost_used = 0

    kwargs_ch = dict(
        q=channel_name,
        part="id,snippet",
        type="channel",
        maxResults=1,
    )
    if region_code:
        kwargs_ch["regionCode"] = region_code
    if lang_code:
        kwargs_ch["relevanceLanguage"] = lang_code

    try:
        ch_resp = youtube.search().list(**kwargs_ch).execute()
        cost_used += 100
    except HttpError as e:
        raise RuntimeError(f"ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    items = ch_resp.get("items", [])
    if not items:
        return [], cost_used

    channel_id = items[0]["id"]["channelId"]

    max_fetch = max(1, min(int(max_fetch or 100), 5000))
    results_tmp = []
    next_token = None
    fetched = 0

    while fetched < max_fetch:
        take = min(50, max_fetch - fetched)
        kwargs = dict(
            part="id",
            type="video",
            channelId=channel_id,
            maxResults=take,
            order="date",
        )
        if published_after:
            kwargs["publishedAfter"] = published_after
        if region_code:
            kwargs["regionCode"] = region_code
        if lang_code:
            kwargs["relevanceLanguage"] = lang_code
        if next_token:
            kwargs["pageToken"] = next_token

        try:
            v_search = youtube.search().list(**kwargs).execute()
            cost_used += 100
        except HttpError as e:
            raise RuntimeError(f"ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        page_ids = [
            it["id"]["videoId"]
            for it in v_search.get("items", [])
            if "id" in it and "videoId" in it["id"]
        ]
        if not page_ids:
            break

        try:
            video_response = youtube.videos().list(
                part="snippet,statistics,contentDetails",
                id=",".join(page_ids),
            ).execute()
            cost_used += 1
        except HttpError as e:
            raise RuntimeError(f"Videos API ì˜¤ë¥˜: {e}")

        items2 = video_response.get("items", [])
        for item in items2:
            vid = item.get("id", "")
            snip = item.get("snippet", {}) or {}
            stats = item.get("statistics", {}) or {}
            cdet = item.get("contentDetails", {}) or {}

            title = snip.get("title", "")
            published_at_iso = snip.get("publishedAt", "")
            view_count = int(stats.get("viewCount", 0))
            url = f"https://www.youtube.com/watch?v={vid}"
            duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

            thumbs = snip.get("thumbnails", {}) or {}
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url")
                or (thumbs.get("standard") or {}).get("url")
                or (thumbs.get("high") or {}).get("url")
                or (thumbs.get("medium") or {}).get("url")
                or (thumbs.get("default") or {}).get("url")
                or ""
            )

            if view_count < min_views:
                continue
            if not duration_filter_ok(duration_sec, duration_label):
                continue

            results_tmp.append(
                {
                    "title": title,
                    "views": view_count,
                    "published_at_iso": published_at_iso,
                    "url": url,
                    "duration_sec": duration_sec,
                    "channel_title": snip.get("channelTitle", ""),
                    "thumbnail_url": thumb_url,
                }
            )

        fetched += len(page_ids)
        next_token = v_search.get("nextPageToken")
        if not next_token:
            break

    return results_tmp, cost_used

def search_trending_videos(
    max_results: int,
    region_code: str | None,
    video_category_id: str | None,
):
    youtube = get_youtube_client()
    take = max(1, min(int(max_results or 50), 50))
    kwargs = dict(
        part="snippet,statistics,contentDetails",
        chart="mostPopular",
        maxResults=take,
    )
    if region_code:
        kwargs["regionCode"] = region_code
    if video_category_id:
        kwargs["videoCategoryId"] = video_category_id

    try:
        resp = youtube.videos().list(**kwargs).execute()
        cost_used = 1
    except HttpError as e:
        raise RuntimeError(f"íŠ¸ë Œë“œ API ì˜¤ë¥˜: {e}")

    results = []
    for item in resp.get("items", []):
        vid = item.get("id", "")
        snip = item.get("snippet", {}) or {}
        stats = item.get("statistics", {}) or {}
        cdet = item.get("contentDetails", {}) or {}

        title = snip.get("title", "")
        published_at_iso = snip.get("publishedAt", "")
        view_count = int(stats.get("viewCount", 0))
        url = f"https://www.youtube.com/watch?v={vid}"
        duration_sec = parse_duration_iso8601(cdet.get("duration", ""))

        thumbs = snip.get("thumbnails", {}) or {}
        thumb_url = (
            (thumbs.get("maxres") or {}).get("url")
            or (thumbs.get("standard") or {}).get("url")
            or (thumbs.get("high") or {}).get("url")
            or (thumbs.get("medium") or {}).get("url")
            or (thumbs.get("default") or {}).get("url")
            or ""
        )

        results.append(
            {
                "title": title,
                "views": view_count,
                "published_at_iso": published_at_iso,
                "url": url,
                "duration_sec": duration_sec,
                "channel_title": snip.get("channelTitle", ""),
                "thumbnail_url": thumb_url,
            }
        )
    return results, cost_used

st.sidebar.caption("ğŸ” YouTubeê²€ìƒ‰ê¸°")

st.session_state.setdefault("sort_key", "ë“±ê¸‰")
st.session_state.setdefault("sort_asc", True)
# ê¸°ë³¸ ë³´ê¸° ëª¨ë“œë¥¼ ë¦¬ìŠ¤íŠ¸ ë·°ë¡œ ì„¤ì •
st.session_state.setdefault("view_mode_label", "ë¦¬ìŠ¤íŠ¸ ë·°")

with st.sidebar.expander("ì •ë ¬ ë°©ì‹", expanded=True):
    sort_key = st.selectbox(
        "ì •ë ¬ ê¸°ì¤€",
        ["ë“±ê¸‰", "ì˜ìƒì¡°íšŒìˆ˜", "ì‹œê°„ë‹¹í´ë¦­", "ì—…ë¡œë“œì‹œê°", "êµ¬ë…ììˆ˜", "ì±„ë„ì¡°íšŒìˆ˜", "ì±„ë„ì˜ìƒìˆ˜"],
        index=0,
        key="sort_key_ui",
    )
    sort_dir = st.radio(
        "ì •ë ¬ ë°©í–¥",
        ["ì˜¤ë¦„ì°¨ìˆœ", "ë‚´ë¦¼ì°¨ìˆœ"],
        index=0 if st.session_state["sort_asc"] else 1,
        horizontal=True,
        key="sort_dir_ui",
    )
    st.session_state["sort_key"] = sort_key
    st.session_state["sort_asc"] = sort_dir == "ì˜¤ë¦„ì°¨ìˆœ"

st.sidebar.markdown("---")

with st.sidebar.expander("âš™ ì„¸ë¶€ í•„í„°", expanded=True):
    api_period = st.selectbox(
        "ì„œë²„ ê²€ìƒ‰ê¸°ê°„ (YouTube API)",
        ["ì œí•œì—†ìŒ", "7ì¼", "30ì¼", "90ì¼", "180ì¼", "365ì¼", "730ì¼"],
        index=1,
        key="api_period",
    )
    upload_period = st.selectbox(
        "ì—…ë¡œë“œ ê¸°ê°„(í´ë¼ì´ì–¸íŠ¸ í•„í„°)",
        ["ì œí•œì—†ìŒ", "1ì¼", "3ì¼", "7ì¼", "30ì¼", "90ì¼", "180ì¼", "365ì¼"],
        index=6,
        key="upload_period",
    )
    min_views_label = st.selectbox(
        "ìµœì†Œ ì¡°íšŒìˆ˜",
        [
            "5,000",
            "10,000",
            "25,000",
            "50,000",
            "100,000",
            "200,000",
            "500,000",
            "1,000,000",
        ],
        index=0,
        key="min_views_label",
    )
    duration_label = st.selectbox(
        "ì˜ìƒ ê¸¸ì´",
        ["ì „ì²´", "ì‡¼ì¸ ", "ë¡±í¼", "1~20ë¶„", "20~40ë¶„", "40~60ë¶„", "60ë¶„ì´ìƒ"],
        index=0,
        key="duration_label",
    )
    max_fetch = st.number_input(
        "ëª¨ë“  ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê°œìˆ˜",
        1,
        5000,
        50,
        step=10,
        key="max_fetch",
    )
    country_name = st.selectbox(
        "ê²€ìƒ‰ìš© êµ­ê°€/ì–¸ì–´", COUNTRY_LIST, index=0, key="country_for_search"
    )
    region_code, lang_code = COUNTRY_LANG_MAP[country_name]

quota_today = get_today_quota_total()
st.sidebar.caption(f"ì˜¤ëŠ˜ ì‚¬ìš© ì¿¼í„°: {quota_today:,} units")

recents = get_recent_keywords(7)
if recents:
    keywords = [q for _, q in recents]
    labels = [f"`{k}`" for k in keywords]
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: " + " Â· ".join(labels))
else:
    st.sidebar.caption("ìµœê·¼ í‚¤ì›Œë“œ: ì—†ìŒ")

status_placeholder = st.empty()

if "results_df" not in st.session_state:
    st.session_state.results_df = None
    st.session_state.last_search_time = None
    st.session_state.search_mode = None

mode_options = ["ì¼ë°˜", "ì±„ë„ì˜ìƒ", "í‚¤ì›Œë“œ ì±„ë„", "íŠ¸ë Œë“œ", "ëœë¤ íŠ¸ë Œë“œ"]
st.session_state.setdefault("search_mode_value", mode_options[0])
st.session_state.setdefault("search_query", "")
st.session_state.setdefault("trend_category_label", list(TREND_CATEGORY_MAP.keys())[0])

do_search = False

with st.expander("ê²€ìƒ‰", expanded=True):
    mode_col, _ = st.columns([1.8, 2.2])
    with mode_col:
        search_mode_label = st.radio(
            "ê²€ìƒ‰ ëª¨ë“œ",
            options=mode_options,
            index=mode_options.index(st.session_state["search_mode_value"]),
            key="search_mode_value",
            horizontal=True,
        )

    if search_mode_label in ("ì¼ë°˜", "ì±„ë„ì˜ìƒ", "í‚¤ì›Œë“œ ì±„ë„"):
        q_col, _ = st.columns([3, 1])
        with q_col:
            search_query = st.text_input(
                "ê²€ìƒ‰ì–´ / ì±„ë„ëª…",
                value=st.session_state["search_query"],
                placeholder="ê²€ìƒ‰ì–´ ë˜ëŠ” ì±„ë„ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.",
                key="search_query_input",
            )
            st.session_state["search_query"] = search_query
    else:
        st.session_state["search_query"] = ""

    if search_mode_label == "íŠ¸ë Œë“œ":
        default_label = st.session_state.get(
            "trend_category_label", list(TREND_CATEGORY_MAP.keys())[0]
        )
        options = list(TREND_CATEGORY_MAP.keys())
        try:
            idx = options.index(default_label)
        except ValueError:
            idx = 0
        trend_category_label = st.selectbox(
            "íŠ¸ë Œë“œ ì¹´í…Œê³ ë¦¬",
            options,
            index=idx,
            key="trend_category_label_widget",
        )
        st.session_state["trend_category_label"] = trend_category_label

    do_search = st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True)

def apply_client_filters(df: pd.DataFrame, upload_period: str, min_views_label: str) -> pd.DataFrame:
    if upload_period != "ì œí•œì—†ìŒ" and "ì—…ë¡œë“œì‹œê°" in df.columns:
        days = int(upload_period.replace("ì¼", ""))
        cutoff = datetime.now(KST) - timedelta(days=days)
        df = df[df["ì—…ë¡œë“œì‹œê°"] >= cutoff]
    min_views = parse_min_views(min_views_label)
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns:
        df = df[df["ì˜ìƒì¡°íšŒìˆ˜"] >= min_views]
    return df

def sort_dataframe(df: pd.DataFrame, mode: str, sort_key: str, ascending: bool) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if sort_key not in df.columns:
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            key_fallback = "ë“±ê¸‰" if "ë“±ê¸‰" in df.columns else None
        else:
            key_fallback = "êµ¬ë…ììˆ˜" if "êµ¬ë…ììˆ˜" in df.columns else None
        if not key_fallback:
            return df
        sort_key = key_fallback

    if sort_key == "ë“±ê¸‰":
        order = ["A", "B", "C", "D", "E", "F", "G", "H"]
        cat = pd.Categorical(df["ë“±ê¸‰"], categories=order, ordered=True)
        df = df.assign(_grade_cat=cat)
        df = df.sort_values("_grade_cat", ascending=ascending, kind="mergesort")
        return df.drop(columns=["_grade_cat"])

    if sort_key in ["ì˜ìƒì¡°íšŒìˆ˜", "ì‹œê°„ë‹¹í´ë¦­"]:
        return df.sort_values(sort_key, ascending=ascending, kind="mergesort")

    if sort_key == "ì—…ë¡œë“œì‹œê°":
        return df.sort_values("ì—…ë¡œë“œì‹œê°", ascending=ascending, kind="mergesort")

    if sort_key in ["êµ¬ë…ììˆ˜", "ì±„ë„ì¡°íšŒìˆ˜", "ì±„ë„ì˜ìƒìˆ˜"]:
        tmp = df[sort_key].astype(str).str.replace(",", "").str.replace(" ", "")
        num = pd.to_numeric(tmp, errors="coerce").fillna(0)
        df = df.assign(_num=num)
        df = df.sort_values("_num", ascending=ascending, kind="mergesort")
        return df.drop(columns=["_num"])

    return df.sort_values(sort_key, ascending=ascending, kind="mergesort")

try:
    mode_triggered = None
    if do_search:
        if search_mode_label == "ëœë¤ íŠ¸ë Œë“œ":
            mode_triggered = "random_trend"
        elif search_mode_label == "ì¼ë°˜":
            mode_triggered = "general"
        elif search_mode_label == "íŠ¸ë Œë“œ":
            mode_triggered = "trend"
        elif search_mode_label == "ì±„ë„ì˜ìƒ":
            mode_triggered = "channel_videos"
        elif search_mode_label == "í‚¤ì›Œë“œ ì±„ë„":
            mode_triggered = "channel_list"

    if mode_triggered is not None:
        search_dt = datetime.now(KST)

        if mode_triggered == "random_trend":
            rand_country_label = random.choice(COUNTRY_LIST)
            rand_region_code, _ = COUNTRY_LANG_MAP[rand_country_label]
            rand_cat_label = random.choice(list(TREND_CATEGORY_MAP.keys()))
            rand_cat_id = TREND_CATEGORY_MAP[rand_cat_label]

            append_keyword_log(f"[random]{rand_country_label}/{rand_cat_label}")
            status_placeholder.info(
                f"ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘... (êµ­ê°€: {rand_country_label}, ì¹´í…Œê³ ë¦¬: {rand_cat_label})"
            )
            raw_results, cost_used = search_trending_videos(
                max_results=max_fetch,
                region_code=rand_region_code,
                video_category_id=rand_cat_id,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "random_trend"
                status_placeholder.info("ëœë¤ íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d * 24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append(
                        {
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        }
                    )
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "random_trend"
                status_placeholder.success(
                    f"[ëœë¤ íŠ¸ë Œë“œ ê²€ìƒ‰] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        elif mode_triggered == "general":
            base_query = (st.session_state.get("search_query") or "").strip()
            if not base_query:
                st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(base_query)
                status_placeholder.info("ì¼ë°˜ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used = search_videos(
                    query=base_query,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "general"
                    status_placeholder.info("ì„œë²„ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d * 24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append(
                            {
                                "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                                "ì±„ë„ëª…": r["channel_title"],
                                "ë“±ê¸‰": calc_grade(cph),
                                "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                                "ì‹œê°„ë‹¹í´ë¦­": cph,
                                "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                                "ì—…ë¡œë“œì‹œê°": pub_kst,
                                "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                                "ì œëª©": r["title"],
                                "ë§í¬URL": r["url"],
                            }
                        )
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "general"
                    status_placeholder.success(
                        f"[ì¼ë°˜ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        elif mode_triggered == "trend":
            options = list(TREND_CATEGORY_MAP.keys())
            trend_label = st.session_state.get("trend_category_label", options[0])
            trend_cat_id = TREND_CATEGORY_MAP.get(trend_label)
            append_keyword_log(f"[trend]{trend_label}")
            status_placeholder.info("íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            raw_results, cost_used = search_trending_videos(
                max_results=max_fetch,
                region_code=region_code,
                video_category_id=trend_cat_id,
            )
            add_quota_usage(cost_used)

            if not raw_results:
                st.session_state.results_df = None
                st.session_state.search_mode = "trend"
                status_placeholder.info("íŠ¸ë Œë“œ ê²°ê³¼ 0ê±´")
            else:
                rows = []
                for r in raw_results:
                    pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                    d, h = human_elapsed_days_hours(search_dt, pub_kst)
                    total_hours = max(1, d * 24 + h)
                    cph = int(round(r["views"] / total_hours))
                    rows.append(
                        {
                            "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                            "ì±„ë„ëª…": r["channel_title"],
                            "ë“±ê¸‰": calc_grade(cph),
                            "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                            "ì‹œê°„ë‹¹í´ë¦­": cph,
                            "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                            "ì—…ë¡œë“œì‹œê°": pub_kst,
                            "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                            "ì œëª©": r["title"],
                            "ë§í¬URL": r["url"],
                        }
                    )
                df = pd.DataFrame(rows)
                if not df.empty:
                    df = apply_client_filters(df, upload_period, min_views_label)
                st.session_state.results_df = df
                st.session_state.last_search_time = search_dt
                st.session_state.search_mode = "trend"
                status_placeholder.success(
                    f"[íŠ¸ë Œë“œ ê²€ìƒ‰] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                )

        elif mode_triggered == "channel_videos":
            ch_name = (st.session_state.get("search_query") or "").strip()
            if not ch_name:
                st.warning("ì±„ë„ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel_videos]{ch_name}")
                status_placeholder.info("ì±„ë„ ì˜ìƒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                raw_results, cost_used = search_videos_in_channel_by_name(
                    channel_name=ch_name,
                    min_views=parse_min_views(min_views_label),
                    api_period_label=api_period,
                    duration_label=duration_label,
                    max_fetch=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not raw_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.info("ì±„ë„ ì˜ìƒ ê²°ê³¼ 0ê±´")
                else:
                    rows = []
                    for r in raw_results:
                        pub_kst = parse_published_at_to_kst(r["published_at_iso"])
                        d, h = human_elapsed_days_hours(search_dt, pub_kst)
                        total_hours = max(1, d * 24 + h)
                        cph = int(round(r["views"] / total_hours))
                        rows.append(
                            {
                                "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                                "ì±„ë„ëª…": r["channel_title"],
                                "ë“±ê¸‰": calc_grade(cph),
                                "ì˜ìƒì¡°íšŒìˆ˜": r["views"],
                                "ì‹œê°„ë‹¹í´ë¦­": cph,
                                "ì˜ìƒê¸¸ì´": format_duration_hms(r["duration_sec"]),
                                "ì—…ë¡œë“œì‹œê°": pub_kst,
                                "ê²½ê³¼ì‹œê°„": f"{d}ì¼ {h}ì‹œê°„",
                                "ì œëª©": r["title"],
                                "ë§í¬URL": r["url"],
                            }
                        )
                    df = pd.DataFrame(rows)
                    if not df.empty:
                        df = apply_client_filters(df, upload_period, min_views_label)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_videos"
                    status_placeholder.success(
                        f"[ì±„ë„ ì˜ìƒ ê²€ìƒ‰] ì„œë²„ ê²°ê³¼: {len(raw_results):,}ê±´ / í•„í„° í›„: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

        elif mode_triggered == "channel_list":
            ch_kw = (st.session_state.get("search_query") or "").strip()
            if not ch_kw:
                st.warning("ì±„ë„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                append_keyword_log(f"[channel]{ch_kw}")
                status_placeholder.info("ì±„ë„ ëª©ë¡ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
                ch_results, cost_used = search_channels_by_keyword(
                    keyword=ch_kw,
                    max_results=max_fetch,
                    region_code=region_code,
                    lang_code=lang_code,
                )
                add_quota_usage(cost_used)

                if not ch_results:
                    st.session_state.results_df = None
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.info("ì±„ë„ ê²°ê³¼ 0ê±´")
                else:
                    df_rows = []
                    for r in ch_results:
                        subs = r["subs"]
                        subs_text = f"{subs:,}" if isinstance(subs, int) else "-"
                        df_rows.append(
                            {
                                "ì¸ë„¤ì¼": r.get("thumbnail_url", ""),
                                "ì±„ë„ëª…": r["channel_title"],
                                "êµ¬ë…ììˆ˜": subs_text,
                                "ì±„ë„ì¡°íšŒìˆ˜": f"{r['total_views']:,}",
                                "ì±„ë„ì˜ìƒìˆ˜": f"{r['videos']:,}",
                                "ë§í¬URL": r["url"],
                            }
                        )
                    df = pd.DataFrame(df_rows)
                    st.session_state.results_df = df
                    st.session_state.last_search_time = search_dt
                    st.session_state.search_mode = "channel_list"
                    status_placeholder.success(
                        f"[ì±„ë„ ëª©ë¡ ê²€ìƒ‰] ê²°ê³¼: {len(df):,}ê±´ (ì´ë²ˆ ì¿¼í„° ì‚¬ìš©ëŸ‰: {cost_used})"
                    )

except Exception as e:
    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    st.session_state.results_df = None

df = st.session_state.results_df
mode = st.session_state.search_mode

if df is None or df.empty:
    st.info("ì•„ì§ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ 'ê²€ìƒ‰'ì„ ì—´ê³  ê²€ìƒ‰ëª¨ë“œì™€ ê²€ìƒ‰ì–´ë¥¼ ì„¤ì •í•œ ë’¤ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
else:
    if "ì˜ìƒì¡°íšŒìˆ˜" in df.columns and "ë“±ê¸‰" in df.columns:
        tmp = df.copy()
        tmp = tmp[tmp["ë“±ê¸‰"].notna()]
        if not tmp.empty:
            tmp_sorted = tmp.sort_values("ì˜ìƒì¡°íšŒìˆ˜", ascending=False)
            tmp_sorted["ìˆœìœ„(ì¡°íšŒìˆ˜ê¸°ì¤€)"] = range(1, len(tmp_sorted) + 1)
            chart_data = tmp_sorted[
                ["ìˆœìœ„(ì¡°íšŒìˆ˜ê¸°ì¤€)", "ì˜ìƒì¡°íšŒìˆ˜", "ì œëª©", "ì±„ë„ëª…", "ë“±ê¸‰"]
            ]
            # ì¡°íšŒìˆ˜ ë¶„í¬ ê·¸ë˜í”„ë¥¼ disclosure(expander) ì•ˆì— ë„£ê³ , ë†’ì´ë¥¼ 3ë°°(540)ë¡œ ì¦ê°€
            with st.expander("ğŸ“ˆ ì¡°íšŒìˆ˜ ë¶„í¬ ê·¸ë˜í”„", expanded=True):
                chart = (
                    alt.Chart(chart_data)
                    .mark_bar()
                    .encode(
                        x=alt.X("ìˆœìœ„(ì¡°íšŒìˆ˜ê¸°ì¤€):O", title="ì¡°íšŒìˆ˜ ê¸°ì¤€ ìˆœìœ„"),
                        y=alt.Y("ì˜ìƒì¡°íšŒìˆ˜:Q", title="ì˜ìƒì¡°íšŒìˆ˜"),
                        tooltip=[
                            alt.Tooltip("ì œëª©:N", title="ì œëª©"),
                            alt.Tooltip("ì±„ë„ëª…:N", title="ì±„ë„"),
                            alt.Tooltip("ì˜ìƒì¡°íšŒìˆ˜:Q", title="ì¡°íšŒìˆ˜", format=",.0f"),
                            alt.Tooltip("ë“±ê¸‰:N", title="ë“±ê¸‰"),
                        ],
                    )
                    .properties(height=540, width="container")
                )
                st.altair_chart(chart, use_container_width=True)
                st.markdown("<div style='margin-top:0.5rem;'></div>", unsafe_allow_html=True)

    # ë³´ê¸° ëª¨ë“œ: ì™¼ìª½ë¶€í„° ë¦¬ìŠ¤íŠ¸ ë·°, ê·¸ë¦¬ë“œ ë·°, ì‡¼ì¸  ë·°
    options_view = ["ë¦¬ìŠ¤íŠ¸ ë·°", "ê·¸ë¦¬ë“œ ë·°", "ì‡¼ì¸  ë·°"]
    default_label = st.session_state.get("view_mode_label", "ë¦¬ìŠ¤íŠ¸ ë·°")
    idx = options_view.index(default_label) if default_label in options_view else 0

    view_mode_label = st.radio(
        "ë³´ê¸° ëª¨ë“œ",
        options=options_view,
        index=idx,
        key="view_mode_label",
        horizontal=True,
    )

    if view_mode_label == "ê·¸ë¦¬ë“œ ë·°":
        view_mode = "grid"
    elif view_mode_label == "ë¦¬ìŠ¤íŠ¸ ë·°":
        view_mode = "list"
    else:
        view_mode = "shorts"

    df_display = df.copy()
    if "ë§í¬URL" in df_display.columns:
        df_display["ë§í¬"] = df_display["ë§í¬URL"]
        df_display = df_display.drop(columns=["ë§í¬URL"])

    df_display = sort_dataframe(
        df_display,
        mode=mode or "",
        sort_key=st.session_state["sort_key"],
        ascending=st.session_state["sort_asc"],
    )

    if mode == "general":
        st.subheader("ğŸ“Š ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode in ("trend", "random_trend"):
        st.subheader("ğŸ”¥ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_videos":
        st.subheader("ğŸ¬ ì±„ë„ ì˜ìƒ ë¦¬ìŠ¤íŠ¸")
    elif mode == "channel_list":
        st.subheader("ğŸ“º ì±„ë„ê²€ìƒ‰ ë¦¬ìŠ¤íŠ¸")
    else:
        st.subheader("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")

    if view_mode == "shorts":
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            html_items = []
            for _, row in df_display.iterrows():
                thumb = str(row.get("ì¸ë„¤ì¼", "") or "")
                if not thumb:
                    continue
                ch = str(row.get("ì±„ë„ëª…", "") or "")
                link = str(row.get("ë§í¬", "") or "")
                channel_html = (
                    f'<div class="shorts-meta-channel">{ch}</div>' if ch else ""
                )
                link_html = (
                    f'<a href="{link}" target="_blank" class="shorts-meta-link">ì˜ìƒ ì—´ê¸°</a>'
                    if link
                    else ""
                )
                meta_html = ""
                if channel_html or link_html:
                    meta_html = f'<div class="shorts-meta">{channel_html}{link_html}</div>'
                html_items.append(
                    '<div class="shorts-item">'
                    f'  <div class="shorts-frame" style="background-image:url(\'{thumb}\');"></div>'
                    f'  {meta_html}'
                    "</div>"
                )
            html = (
                "<style>"
                ".shorts-container{display:flex;flex-wrap:wrap;justify-content:center;gap:4px 4px;}"
                ".shorts-item{flex:0 0 23%;max-width:170px;}"
                ".shorts-frame{position:relative;width:100%;height:0;padding-bottom:177%;"
                "overflow:hidden;border-radius:10px;background:#000;"
                "background-size:cover;background-position:center center;background-repeat:no-repeat;}"
                ".shorts-meta{display:flex;justify-content:space-between;align-items:center;"
                "margin-top:2px;font-size:11px;line-height:1.2;}"
                ".shorts-meta-channel{flex:1;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;"
                "padding-right:4px;}"
                ".shorts-meta-link{text-decoration:none;border:1px solid #ccc;border-radius:999px;"
                "padding:1px 6px;font-size:11px;}"
                "@media (max-width:480px){"
                ".shorts-item{flex:0 0 48%;max-width:none;}"
                ".shorts-container{gap:4px 4px;}"
                "}"
                "</style>"
                f'<div class="shorts-container">{"".join(html_items)}</div>'
            )
            st.markdown(html, unsafe_allow_html=True)

        elif mode == "channel_list":
            thumbs = df_display["ì¸ë„¤ì¼"].astype(str).tolist()
            html_items = []
            for url in thumbs:
                if not url:
                    continue
                html_items.append(
                    '<div class="shorts-item">'
                    f'  <img src="{url}" class="channel-icon"/>'
                    "</div>"
                )
            html = (
                "<style>"
                ".shorts-container{display:flex;flex-wrap:wrap;justify-content:center;gap:6px 6px;}"
                ".shorts-item{flex:0 0 22%;max-width:100px;}"
                ".channel-icon{width:100px;height:100px;object-fit:cover;border-radius:50%;display:block;}"
                "@media (max-width:480px){"
                ".shorts-item{flex:0 0 25%;}"
                ".channel-icon{width:80px;height:80px;}"
                "}"
                "</style>"
                f'<div class="shorts-container">{"".join(html_items)}</div>'
            )
            st.markdown(html, unsafe_allow_html=True)

        st.caption("ì‡¼ì¸  ë·°: ì´ë¯¸ì§€ë¥¼ ëˆŒëŸ¬ë„ ë³„ë„ ë™ì‘ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    elif view_mode == "grid":
        n_cols = 3
        cols = st.columns(n_cols)

        for idx, (_, row) in enumerate(df_display.iterrows()):
            col = cols[idx % n_cols]
            with col:
                if (
                    "ì¸ë„¤ì¼" in df_display.columns
                    and isinstance(row["ì¸ë„¤ì¼"], str)
                    and row["ì¸ë„¤ì¼"]
                ):
                    st.image(row["ì¸ë„¤ì¼"], use_column_width=True)

                if mode == "channel_list":
                    title = row.get("ì±„ë„ëª…", "")
                    subs = row.get("êµ¬ë…ììˆ˜", "")
                    total_views = row.get("ì±„ë„ì¡°íšŒìˆ˜", "")
                    video_count = row.get("ì±„ë„ì˜ìƒìˆ˜", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    st.caption(f"êµ¬ë…ì: {subs} Â· ì¡°íšŒìˆ˜: {total_views} Â· ì˜ìƒìˆ˜: {video_count}")
                    if link:
                        st.markdown(f"[ì±„ë„ ì—´ê¸°]({link})")
                else:
                    title = row.get("ì œëª©", "")
                    ch = row.get("ì±„ë„ëª…", "")
                    views = row.get("ì˜ìƒì¡°íšŒìˆ˜", "")
                    grade = row.get("ë“±ê¸‰", "")
                    link = row.get("ë§í¬", "")
                    st.markdown(f"**{title}**")
                    if isinstance(views, int):
                        st.caption(f"ë“±ê¸‰ {grade} Â· {ch} Â· ì¡°íšŒìˆ˜ {views:,}")
                    else:
                        st.caption(f"ë“±ê¸‰ {grade} Â· {ch} Â· ì¡°íšŒìˆ˜ {views}")
                    if link:
                        st.markdown(f"[ì˜ìƒ ì—´ê¸°]({link})")

            if (idx + 1) % n_cols == 0 and (idx + 1) < len(df_display):
                cols = st.columns(n_cols)

        st.caption("ğŸ‘‰ í…ìŠ¤íŠ¸ ë§í¬ë¥¼ ëˆŒëŸ¬ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì„ ì—´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    else:
        # ë¦¬ìŠ¤íŠ¸ ë·°ì¼ ë•Œì˜ ì»¬ëŸ¼ ìˆœì„œ ì •ì˜
        if mode in ("general", "trend", "random_trend", "channel_videos"):
            # ì—´ê¸°(ë§í¬) ì»¬ëŸ¼ì„ ë“±ê¸‰ ì™¼ìª½ìœ¼ë¡œ ì´ë™
            base_order = [
                "ë§í¬",
                "ë“±ê¸‰",
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "ì˜ìƒì¡°íšŒìˆ˜",
                "ì‹œê°„ë‹¹í´ë¦­",
                "ì˜ìƒê¸¸ì´",
                "ì—…ë¡œë“œì‹œê°",
                "ê²½ê³¼ì‹œê°„",
                "ì œëª©",
            ]
        else:
            base_order = [
                "ì¸ë„¤ì¼",
                "ì±„ë„ëª…",
                "êµ¬ë…ììˆ˜",
                "ì±„ë„ì¡°íšŒìˆ˜",
                "ì±„ë„ì˜ìƒìˆ˜",
                "ë§í¬",
            ]
        column_order = [c for c in base_order if c in df_display.columns]

        column_config = {}
        if "ë§í¬" in df_display.columns:
            column_config["ë§í¬"] = st.column_config.LinkColumn(
                "ì—´ê¸°",
                display_text="ì—´ê¸°",
            )
        if "ì¸ë„¤ì¼" in df_display.columns:
            column_config["ì¸ë„¤ì¼"] = st.column_config.ImageColumn(
                "ì¸ë„¤ì¼",
                help="ì¸ë„¤ì¼ ì´ë¯¸ì§€",
                width="small",
            )
        if "ì—…ë¡œë“œì‹œê°" in df_display.columns:
            column_config["ì—…ë¡œë“œì‹œê°"] = st.column_config.DatetimeColumn(
                "ì—…ë¡œë“œì‹œê°",
                format="YYYY-MM-DD HH:mm",
            )

        if mode == "general":
            editor_key = "video_results_editor_general"
        elif mode in ("trend", "random_trend"):
            editor_key = "video_results_editor_trend"
        elif mode == "channel_videos":
            editor_key = "video_results_editor_channel_videos"
        elif mode == "channel_list":
            editor_key = "channel_results_editor_keyword"
        else:
            editor_key = "results_editor_default"

        st.data_editor(
            df_display,
            use_container_width=True,
            height=620,
            hide_index=True,
            column_order=column_order if column_order else None,
            column_config=column_config,
            key=editor_key,
            disabled=True,
            num_rows="fixed",
        )

        st.caption("ğŸ‘‰ 'ì—´ê¸°' ë§í¬ë¥¼ ëˆ„ë¥´ë©´ ìƒˆ íƒ­ì—ì„œ ì˜ìƒ ë˜ëŠ” ì±„ë„ì´ ì—´ë¦½ë‹ˆë‹¤.")

st.markdown(
    """
<div style="text-align:center;margin:1.5rem 0 2.5rem;">
  <a href="#page_top"
     style="display:inline-block;padding:0.5rem 1.2rem;border-radius:999px;
            border:1px solid #ccc;text-decoration:none;font-size:13px;">
    â¬† í˜ì´ì§€ ìƒë‹¨ìœ¼ë¡œ
  </a>
</div>
""",
    unsafe_allow_html=True,
)
